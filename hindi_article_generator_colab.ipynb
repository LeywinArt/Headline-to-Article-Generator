{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d502abc",
   "metadata": {},
   "source": [
    "## Step 1: Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e5ed5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# Install Unsloth for efficient LLM fine-tuning\n",
    "!pip install \"unsloth[colab-new] @ git+https://github.com/unslothai/unsloth.git\"\n",
    "!pip install --no-deps xformers trl peft accelerate bitsandbytes triton\n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd7e689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify installations and GPU\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è No GPU detected! Go to Runtime ‚Üí Change runtime type ‚Üí GPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fcd33e0",
   "metadata": {},
   "source": [
    "## Step 2: Upload Dataset\n",
    "Upload the `bbc_hindi_articles_with_categories_cleaned.csv` file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04fb172e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import files\n",
    "import os\n",
    "\n",
    "# Check if file already exists\n",
    "csv_path = \"bbc_hindi_articles_with_categories_cleaned.csv\"\n",
    "\n",
    "if not os.path.exists(csv_path):\n",
    "    print(\"Please upload the CSV file:\")\n",
    "    uploaded = files.upload()\n",
    "    if csv_path in uploaded:\n",
    "        print(f\"‚úì File uploaded successfully!\")\n",
    "else:\n",
    "    print(f\"‚úì File already exists: {csv_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d49b39",
   "metadata": {},
   "source": [
    "## Step 3: Load the Model\n",
    "Using Unsloth to load Llama-3 8B in 4-bit quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb34f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "import torch\n",
    "\n",
    "max_seq_length = 2048\n",
    "dtype = None  # Auto-detect\n",
    "load_in_4bit = True  # Use 4-bit quantization to reduce memory\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"unsloth/llama-3-8b-bnb-4bit\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dtype=dtype,\n",
    "    load_in_4bit=load_in_4bit,\n",
    ")\n",
    "\n",
    "print(\"‚úì Model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5398e7ea",
   "metadata": {},
   "source": [
    "## Step 4: Configure LoRA Adapters\n",
    "Setting up Low-Rank Adaptation for efficient fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120b8bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,  # LoRA rank\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                    \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_alpha=16,\n",
    "    lora_dropout=0,\n",
    "    bias=\"none\",\n",
    "    use_gradient_checkpointing=\"unsloth\",\n",
    "    random_state=3407,\n",
    "    use_rslora=False,\n",
    "    loftq_config=None,\n",
    ")\n",
    "\n",
    "print(\"‚úì LoRA adapters configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e2fe5b",
   "metadata": {},
   "source": [
    "## Step 5: Prepare the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a668f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "def formatting_prompts_func(example):\n",
    "    \"\"\"Format data into training prompts\"\"\"\n",
    "    output_texts = []\n",
    "    for i in range(len(example['Headline'])):\n",
    "        text = f\"### Headline: {example['Headline'][i]}\\n ### Category: {example['Category'][i]}  ### Article: {example['Content'][i]}\"\n",
    "        output_texts.append(text)\n",
    "    return {\"text\": output_texts}\n",
    "\n",
    "# Load dataset\n",
    "csv_path = \"bbc_hindi_articles_with_categories_cleaned.csv\"\n",
    "dataset = load_dataset('csv', data_files=csv_path, split=\"train\")\n",
    "dataset = dataset.map(formatting_prompts_func, batched=True)\n",
    "\n",
    "print(f\"‚úì Dataset loaded: {len(dataset)} samples\")\n",
    "print(f\"\\nSample headline: {dataset[0]['Headline'][:80]}...\")\n",
    "print(f\"Category: {dataset[0]['Category']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08bcd9a4",
   "metadata": {},
   "source": [
    "## Step 6: Setup Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f425eab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import SFTTrainer\n",
    "from transformers import TrainingArguments\n",
    "import os\n",
    "\n",
    "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    train_dataset=dataset,\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=max_seq_length,\n",
    "    dataset_num_proc=2,\n",
    "    packing=False,\n",
    "    args=TrainingArguments(\n",
    "        num_train_epochs=1,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=4,\n",
    "        warmup_steps=5,\n",
    "        learning_rate=2e-4,\n",
    "        fp16=not torch.cuda.is_bf16_supported(),\n",
    "        bf16=torch.cuda.is_bf16_supported(),\n",
    "        logging_steps=10,\n",
    "        optim=\"adamw_8bit\",\n",
    "        weight_decay=0.01,\n",
    "        lr_scheduler_type=\"linear\",\n",
    "        seed=3407,\n",
    "        output_dir=\"outputs\",\n",
    "        report_to=\"none\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "print(\"‚úì Trainer configured!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40c386db",
   "metadata": {},
   "source": [
    "## Step 7: Check GPU Memory Before Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef7bc157",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu_stats = torch.cuda.get_device_properties(0)\n",
    "start_gpu_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "max_memory = round(gpu_stats.total_memory / 1024 / 1024 / 1024, 3)\n",
    "\n",
    "print(f\"GPU: {gpu_stats.name}\")\n",
    "print(f\"Max memory: {max_memory} GB\")\n",
    "print(f\"Memory reserved: {start_gpu_memory} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3d6ef01",
   "metadata": {},
   "source": [
    "## Step 8: Train the Model üöÄ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e157d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting training...\")\n",
    "trainer_stats = trainer.train()\n",
    "print(\"\\n‚úì Training complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd6edae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Summary\n",
    "used_memory = round(torch.cuda.max_memory_reserved() / 1024 / 1024 / 1024, 3)\n",
    "used_memory_for_lora = round(used_memory - start_gpu_memory, 3)\n",
    "used_percentage = round(used_memory / max_memory * 100, 3)\n",
    "lora_percentage = round(used_memory_for_lora / max_memory * 100, 3)\n",
    "\n",
    "print(f\"Training time: {trainer_stats.metrics['train_runtime']:.1f} seconds\")\n",
    "print(f\"Training time: {trainer_stats.metrics['train_runtime']/60:.2f} minutes\")\n",
    "print(f\"Peak memory: {used_memory} GB ({used_percentage}%)\")\n",
    "print(f\"Memory for LoRA training: {used_memory_for_lora} GB ({lora_percentage}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba31c48",
   "metadata": {},
   "source": [
    "## Step 9: Generate Hindi Articles! üìù\n",
    "Now let's test the fine-tuned model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02fc7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch to inference mode\n",
    "FastLanguageModel.for_inference(model)\n",
    "\n",
    "# Test headline 1\n",
    "headline = \"‡§≠‡§æ‡§∞‡§§‡•Ä‡§Ø ‡§∂‡•á‡§Ø‡§∞ ‡§¨‡§æ‡§ú‡§æ‡§∞ ‡§Æ‡•á‡§Ç ‡§§‡•á‡§ú‡•Ä\"\n",
    "category = \"‡§≠‡§æ‡§∞‡§§\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "    [f\"### Headline: {headline}\\n ### Category: {category}  ### Article: \"],\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(**inputs, max_new_tokens=128, use_cache=True)\n",
    "generated_text = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"Headline: {headline}\")\n",
    "print(f\"Category: {category}\")\n",
    "print(\"=\"*60)\n",
    "print(\"Generated Article:\")\n",
    "print(generated_text.split(\"### Article:\")[1] if \"### Article:\" in generated_text else generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3642ff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test headline 2 with sampling\n",
    "headline = \"‡§™‡•Ä‡§è‡§Æ ‡§Æ‡•ã‡§¶‡•Ä ‡§Ö‡§´‡•ç‡§∞‡•Ä‡§ï‡§æ ‡§¶‡•å‡§∞‡•á ‡§™‡§∞ ‡§ó‡§è\"\n",
    "category = \"‡§≠‡§æ‡§∞‡§§\"\n",
    "\n",
    "inputs = tokenizer(\n",
    "    [f\"### Headline: {headline}\\n ### Category: {category}  ### Article: \"],\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=256,\n",
    "    use_cache=True,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    temperature=0.8,\n",
    ")\n",
    "generated_text = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"Headline: {headline}\")\n",
    "print(f\"Category: {category}\")\n",
    "print(\"=\"*60)\n",
    "print(\"Generated Article:\")\n",
    "print(generated_text.split(\"### Article:\")[1] if \"### Article:\" in generated_text else generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdaa48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try your own headline!\n",
    "headline = \"‡§ï‡•ç‡§∞‡§ø‡§ï‡•á‡§ü ‡§µ‡§ø‡§∂‡•ç‡§µ ‡§ï‡§™ ‡§Æ‡•á‡§Ç ‡§≠‡§æ‡§∞‡§§ ‡§ï‡•Ä ‡§ú‡•Ä‡§§\"  # Change this!\n",
    "category = \"‡§ñ‡•á‡§≤\"  # Change this!\n",
    "\n",
    "inputs = tokenizer(\n",
    "    [f\"### Headline: {headline}\\n ### Category: {category}  ### Article: \"],\n",
    "    return_tensors=\"pt\"\n",
    ").to(\"cuda\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_new_tokens=256,\n",
    "    use_cache=True,\n",
    "    do_sample=True,\n",
    "    top_p=0.9,\n",
    "    temperature=0.7,\n",
    ")\n",
    "generated_text = tokenizer.batch_decode(outputs)[0]\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(f\"Headline: {headline}\")\n",
    "print(f\"Category: {category}\")\n",
    "print(\"=\"*60)\n",
    "print(\"Generated Article:\")\n",
    "print(generated_text.split(\"### Article:\")[1] if \"### Article:\" in generated_text else generated_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bf50cd8",
   "metadata": {},
   "source": [
    "## Step 10: Save the Model (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8d4b0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save LoRA adapters locally\n",
    "model.save_pretrained(\"hindi_article_lora\")\n",
    "tokenizer.save_pretrained(\"hindi_article_lora\")\n",
    "print(\"‚úì Model saved to 'hindi_article_lora'\")\n",
    "\n",
    "# Download the saved model\n",
    "from google.colab import files\n",
    "!zip -r hindi_article_lora.zip hindi_article_lora\n",
    "files.download('hindi_article_lora.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9503a32",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated:\n",
    "- Loading Llama-3 8B with 4-bit quantization using Unsloth\n",
    "- Fine-tuning with LoRA adapters on Hindi news data\n",
    "- Generating Hindi articles from headlines\n",
    "\n",
    "**Other possible tasks with this dataset:**\n",
    "- Generating headlines from articles\n",
    "- Article classification by category\n",
    "- Headline classification by category"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
